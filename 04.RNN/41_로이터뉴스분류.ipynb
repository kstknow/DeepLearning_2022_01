{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "41.로이터뉴스분류.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 로이터 뉴스 분류하기 - LSTM\n",
        "- 케라스에서 데이터셋 불러온다."
      ],
      "metadata": {
        "id": "9cAUoPootlvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "97UfDnDPts6K"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data() # 튜플로 받는다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7WeEzxsuJ6o",
        "outputId": "06197951-4913-46c2-ff14-bbad13228f39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n",
            "2121728/2110848 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 갯수\n",
        "len(X_train), len(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llp8g0hLukXK",
        "outputId": "a09dc9ca-f450-4f59-eb2c-9d54861ab9d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982, 2246)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#카테고리 갯수\n",
        "max(y_train) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od4eI692uv6p",
        "outputId": "f3e99199-d59d-4b03-f96d-1feb953b3b48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 뉴스\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_3UVLYEvJG5",
        "outputId": "22fe635b-97ad-4826-e55e-ca0f4775a674"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(reuters.get_word_index()), reuters.get_word_index()['the']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MV-f9wxvN4B",
        "outputId": "04694b02-3220-4682-f7b1-dceb6844cfb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "565248/550378 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_word = {}\n",
        "for key, value in reuters.get_word_index().items():\n",
        "    index_word[value] = key\n",
        "len(index_word)\n",
        "# value와 키를 바꿈"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BsxTqD6vkuq",
        "outputId": "1359e439-c493-4f99-ba7d-37bcc3e55fb9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30979"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 기사 복원\n",
        "' '.join(index_word[index] for index in X_train[0])\n",
        "# 파이썬에서 많이 사용/ 리스트 표현식, join은 꼭 기억해라"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "eeqWPEYGv1TS",
        "outputId": "2310f90f-97b1-4aa3-94e1-0f9828daae7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 뉴스 기사의 길이\n",
        "print('뉴스 최대길이: ', max(len(s) for s in X_train))\n",
        "print('뉴스 평균길이: ', sum(map(len, X_train)) /len(X_train)) #len함수를 매핑해서 합을 도출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ954035wa27",
        "outputId": "ba3bbdda-106a-4e14-d649-20d59090c706"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴스 최대길이:  2376\n",
            "뉴스 평균길이:  145.5398574927633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = [[1,2,3],[4,5,6,7],[8,9]]\n",
        "list(map(len, x))\n",
        "sum(map(len, x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2SyfKPjxtNb",
        "outputId": "ccb9ce1b-938b-4a86-cfa3-c605a7a78619"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대부분의 기사는 앞쪽에 있다\n",
        "plt.hist([len(s) for s in X_train], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "n7GMInKpyCmK",
        "outputId": "709b04c1-2a25-4811-e1c2-a260765edd80"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "sns.countplot(y_train);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "J6mge9lxyP5i",
        "outputId": "56ea52e7-2dc4-46fa-ca02-4dca96672f2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터가 0에서 45까지 있다. 분포가 고르지 못하고 3,4기사 압도적으로 많음"
      ],
      "metadata": {
        "id": "GTlcRwi1yppE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM으로 로이터 뉴스 분류\n",
        "- 단어 : 빈도수 기준 1,000 단어 (총 단어수: 30,979)\n",
        "- 길이 : 100단어까지 (최대 2,376)  \n",
        "- LSTM에서는 단어의 갯수를 맞춰 진행해야된다."
      ],
      "metadata": {
        "id": "18P2JNBpzBU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 1000\n",
        "max_len = 100"
      ],
      "metadata": {
        "id": "wQAQhEVY0zxV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "seed = 2022\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "gEWpXuEYzeFL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "metadata": {
        "id": "7nPc-7rLzuKC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = num_words,  test_split = 0.2)\n",
        "# 미리 문장길이를 제한 하지 말고 다 뽑아 놓고 pad_sequences에서 뽑아라\n",
        "# max_len을 100으로 설정했기 때문에 뉴스의 길이가 50이면 100을 안넘어서 설정이 안됨. 짧은 문장은 0padding 긴 문장은 짤리고"
      ],
      "metadata": {
        "id": "57o_GpQh0QB6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen= max_len)\n",
        "Y_train = to_categorical(y_train)\n",
        "Y_test = to_categorical(y_test)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4H5PFL21Gp0",
        "outputId": "644978af-38ce-448e-f204-506258504490"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8982, 100), (2246, 100), (8982, 46), (2246, 46))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 정의/설정/학습"
      ],
      "metadata": {
        "id": "o3PCeYzh2k-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n"
      ],
      "metadata": {
        "id": "x_HRbakM1mhL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "                    Embedding(num_words, embedding_dim, input_length = max_len),\n",
        "                    LSTM(hidden_units),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF9R2otR2ini",
        "outputId": "4926e6ba-34e7-44f4-a2d7-4a02a14eb12b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 128)          128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 46)                5934      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 265,518\n",
            "Trainable params: 265,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile('adam', 'categorical_crossentropy', ['accuracy'])\n"
      ],
      "metadata": {
        "id": "6vLkwHOf3rJS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'best-reuters-lstm.h5'\n",
        "mc = ModelCheckpoint(model_path, save_best_only= True, verbose = 1 )\n",
        "es = EarlyStopping(patience= 5)"
      ],
      "metadata": {
        "id": "mZaZ0YgT37Ob"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(\n",
        "    X_train, Y_train ,epochs = 30, batch_size=128,\n",
        "    validation_split = 0.2, callbacks=[mc, es]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNYGNVKh4NOm",
        "outputId": "5f893db2-4e88-4753-b32a-35ba05d49a80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "57/57 [==============================] - ETA: 0s - loss: 2.6760 - accuracy: 0.3354\n",
            "Epoch 00001: val_loss improved from inf to 2.31994, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 12s 66ms/step - loss: 2.6760 - accuracy: 0.3354 - val_loss: 2.3199 - val_accuracy: 0.3450\n",
            "Epoch 2/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 2.0393 - accuracy: 0.4598\n",
            "Epoch 00002: val_loss improved from 2.31994 to 1.94407, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 41ms/step - loss: 2.0391 - accuracy: 0.4597 - val_loss: 1.9441 - val_accuracy: 0.4953\n",
            "Epoch 3/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.8838 - accuracy: 0.5013\n",
            "Epoch 00003: val_loss improved from 1.94407 to 1.88775, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 35ms/step - loss: 1.8836 - accuracy: 0.5012 - val_loss: 1.8878 - val_accuracy: 0.3973\n",
            "Epoch 4/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.7690 - accuracy: 0.5202\n",
            "Epoch 00004: val_loss improved from 1.88775 to 1.75030, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.7680 - accuracy: 0.5214 - val_loss: 1.7503 - val_accuracy: 0.5415\n",
            "Epoch 5/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.6516 - accuracy: 0.5730\n",
            "Epoch 00005: val_loss improved from 1.75030 to 1.66272, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 1.6519 - accuracy: 0.5730 - val_loss: 1.6627 - val_accuracy: 0.5876\n",
            "Epoch 6/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.6512 - accuracy: 0.5784\n",
            "Epoch 00006: val_loss improved from 1.66272 to 1.63609, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.6521 - accuracy: 0.5784 - val_loss: 1.6361 - val_accuracy: 0.5843\n",
            "Epoch 7/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.5163 - accuracy: 0.6131\n",
            "Epoch 00007: val_loss improved from 1.63609 to 1.56381, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.5154 - accuracy: 0.6134 - val_loss: 1.5638 - val_accuracy: 0.6071\n",
            "Epoch 8/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.4341 - accuracy: 0.6320\n",
            "Epoch 00008: val_loss improved from 1.56381 to 1.52223, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.4334 - accuracy: 0.6322 - val_loss: 1.5222 - val_accuracy: 0.6110\n",
            "Epoch 9/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.3620 - accuracy: 0.6520\n",
            "Epoch 00009: val_loss improved from 1.52223 to 1.44836, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.3613 - accuracy: 0.6526 - val_loss: 1.4484 - val_accuracy: 0.6455\n",
            "Epoch 10/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.2776 - accuracy: 0.6703\n",
            "Epoch 00010: val_loss improved from 1.44836 to 1.39512, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.2772 - accuracy: 0.6704 - val_loss: 1.3951 - val_accuracy: 0.6583\n",
            "Epoch 11/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.2025 - accuracy: 0.6918\n",
            "Epoch 00011: val_loss did not improve from 1.39512\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 1.2029 - accuracy: 0.6919 - val_loss: 1.4059 - val_accuracy: 0.6578\n",
            "Epoch 12/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 1.1759 - accuracy: 0.7002\n",
            "Epoch 00012: val_loss improved from 1.39512 to 1.39012, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.1761 - accuracy: 0.7001 - val_loss: 1.3901 - val_accuracy: 0.6578\n",
            "Epoch 13/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.1992 - accuracy: 0.6905\n",
            "Epoch 00013: val_loss improved from 1.39012 to 1.36977, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 1.1987 - accuracy: 0.6909 - val_loss: 1.3698 - val_accuracy: 0.6694\n",
            "Epoch 14/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.0947 - accuracy: 0.7175\n",
            "Epoch 00014: val_loss improved from 1.36977 to 1.30711, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.0917 - accuracy: 0.7187 - val_loss: 1.3071 - val_accuracy: 0.6895\n",
            "Epoch 15/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 1.0410 - accuracy: 0.7304\n",
            "Epoch 00015: val_loss improved from 1.30711 to 1.29733, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 1.0460 - accuracy: 0.7296 - val_loss: 1.2973 - val_accuracy: 0.6861\n",
            "Epoch 16/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 0.9833 - accuracy: 0.7527\n",
            "Epoch 00016: val_loss did not improve from 1.29733\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 0.9824 - accuracy: 0.7528 - val_loss: 1.3184 - val_accuracy: 0.6895\n",
            "Epoch 17/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 0.9578 - accuracy: 0.7573\n",
            "Epoch 00017: val_loss improved from 1.29733 to 1.28527, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 0.9569 - accuracy: 0.7574 - val_loss: 1.2853 - val_accuracy: 0.6989\n",
            "Epoch 18/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 0.9148 - accuracy: 0.7719\n",
            "Epoch 00018: val_loss improved from 1.28527 to 1.27698, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 2s 30ms/step - loss: 0.9144 - accuracy: 0.7720 - val_loss: 1.2770 - val_accuracy: 0.6973\n",
            "Epoch 19/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 0.8631 - accuracy: 0.7827\n",
            "Epoch 00019: val_loss did not improve from 1.27698\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 0.8622 - accuracy: 0.7826 - val_loss: 1.2852 - val_accuracy: 0.7073\n",
            "Epoch 20/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 0.8356 - accuracy: 0.7905\n",
            "Epoch 00020: val_loss did not improve from 1.27698\n",
            "57/57 [==============================] - 2s 28ms/step - loss: 0.8357 - accuracy: 0.7905 - val_loss: 1.3152 - val_accuracy: 0.6939\n",
            "Epoch 21/30\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.8083 - accuracy: 0.7961\n",
            "Epoch 00021: val_loss did not improve from 1.27698\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 0.8083 - accuracy: 0.7961 - val_loss: 1.2947 - val_accuracy: 0.7134\n",
            "Epoch 22/30\n",
            "55/57 [===========================>..] - ETA: 0s - loss: 0.7553 - accuracy: 0.8116\n",
            "Epoch 00022: val_loss did not improve from 1.27698\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 0.7577 - accuracy: 0.8117 - val_loss: 1.3033 - val_accuracy: 0.7123\n",
            "Epoch 23/30\n",
            "56/57 [============================>.] - ETA: 0s - loss: 0.7520 - accuracy: 0.8064\n",
            "Epoch 00023: val_loss did not improve from 1.27698\n",
            "57/57 [==============================] - 2s 29ms/step - loss: 0.7537 - accuracy: 0.8063 - val_loss: 1.3025 - val_accuracy: 0.7090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model(model_path)\n",
        "best_model.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-wPp2764gvS",
        "outputId": "bf3e97a9-1a70-4382-923d-cf1265b34afd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 2s 12ms/step - loss: 1.3044 - accuracy: 0.6825\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3043806552886963, 0.6825467348098755]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- case 2) : max_len이 300일 경우"
      ],
      "metadata": {
        "id": "CcwLIe207II9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words = num_words,  test_split = 0.2)"
      ],
      "metadata": {
        "id": "G1QWv93Y7O4s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 300\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen= max_len)\n",
        "Y_train = to_categorical(y_train)\n",
        "Y_test = to_categorical(y_test)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URBQhNEc7gQh",
        "outputId": "e9be284f-fb49-479d-e962-aba013499bfb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8982, 300), (2246, 300), (8982, 46), (2246, 46))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential([\n",
        "                    Embedding(num_words, embedding_dim, input_length = max_len),\n",
        "                    LSTM(hidden_units),\n",
        "                    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDotPm-e7n-U",
        "outputId": "130f33db-8460-4de4-ca85-27a54a572118"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 300, 128)          128000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 46)                5934      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 265,518\n",
            "Trainable params: 265,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'best-reuters-lstm.h5'\n",
        "mc = ModelCheckpoint(model_path, save_best_only= True, verbose = 1 )\n",
        "es = EarlyStopping(patience= 10)"
      ],
      "metadata": {
        "id": "cmRYf5cp8VdF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile('adam', 'categorical_crossentropy', ['accuracy'])\n"
      ],
      "metadata": {
        "id": "Zcp9uMAa7rlO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model2.fit(\n",
        "    X_train, Y_train ,epochs = 50, batch_size=128,\n",
        "    validation_split = 0.2, callbacks=[mc, es]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYeCel87zpR",
        "outputId": "b55587bc-1f26-47c8-ae58-b533cc4755fb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 2.6729 - accuracy: 0.3470\n",
            "Epoch 00001: val_loss improved from inf to 2.42792, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 8s 85ms/step - loss: 2.6729 - accuracy: 0.3470 - val_loss: 2.4279 - val_accuracy: 0.3450\n",
            "Epoch 2/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 2.2145 - accuracy: 0.4420\n",
            "Epoch 00002: val_loss improved from 2.42792 to 2.09838, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 2.2145 - accuracy: 0.4420 - val_loss: 2.0984 - val_accuracy: 0.4691\n",
            "Epoch 3/50\n",
            "56/57 [============================>.] - ETA: 0s - loss: 2.0316 - accuracy: 0.4961\n",
            "Epoch 00003: val_loss improved from 2.09838 to 2.01480, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 2.0313 - accuracy: 0.4962 - val_loss: 2.0148 - val_accuracy: 0.4853\n",
            "Epoch 4/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.9030 - accuracy: 0.5193\n",
            "Epoch 00004: val_loss improved from 2.01480 to 1.85749, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.9030 - accuracy: 0.5193 - val_loss: 1.8575 - val_accuracy: 0.5326\n",
            "Epoch 5/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.7773 - accuracy: 0.5381\n",
            "Epoch 00005: val_loss improved from 1.85749 to 1.71831, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.7773 - accuracy: 0.5381 - val_loss: 1.7183 - val_accuracy: 0.5576\n",
            "Epoch 6/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.6874 - accuracy: 0.5656\n",
            "Epoch 00006: val_loss improved from 1.71831 to 1.71633, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.6874 - accuracy: 0.5656 - val_loss: 1.7163 - val_accuracy: 0.5554\n",
            "Epoch 7/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.6530 - accuracy: 0.5716\n",
            "Epoch 00007: val_loss improved from 1.71633 to 1.64591, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.6530 - accuracy: 0.5716 - val_loss: 1.6459 - val_accuracy: 0.5782\n",
            "Epoch 8/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.6093 - accuracy: 0.5864\n",
            "Epoch 00008: val_loss did not improve from 1.64591\n",
            "57/57 [==============================] - 4s 75ms/step - loss: 1.6093 - accuracy: 0.5864 - val_loss: 1.6682 - val_accuracy: 0.5721\n",
            "Epoch 9/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.5944 - accuracy: 0.5930\n",
            "Epoch 00009: val_loss improved from 1.64591 to 1.56514, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 77ms/step - loss: 1.5944 - accuracy: 0.5930 - val_loss: 1.5651 - val_accuracy: 0.6177\n",
            "Epoch 10/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.4547 - accuracy: 0.6285\n",
            "Epoch 00010: val_loss improved from 1.56514 to 1.52909, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.4547 - accuracy: 0.6285 - val_loss: 1.5291 - val_accuracy: 0.6110\n",
            "Epoch 11/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.3909 - accuracy: 0.6498\n",
            "Epoch 00011: val_loss improved from 1.52909 to 1.37996, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.3909 - accuracy: 0.6498 - val_loss: 1.3800 - val_accuracy: 0.6672\n",
            "Epoch 12/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.3421 - accuracy: 0.6628\n",
            "Epoch 00012: val_loss did not improve from 1.37996\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.3421 - accuracy: 0.6628 - val_loss: 1.4202 - val_accuracy: 0.6472\n",
            "Epoch 13/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.3432 - accuracy: 0.6605\n",
            "Epoch 00013: val_loss did not improve from 1.37996\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.3432 - accuracy: 0.6605 - val_loss: 1.3977 - val_accuracy: 0.6550\n",
            "Epoch 14/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.2529 - accuracy: 0.6831\n",
            "Epoch 00014: val_loss improved from 1.37996 to 1.33471, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.2529 - accuracy: 0.6831 - val_loss: 1.3347 - val_accuracy: 0.6533\n",
            "Epoch 15/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.3297 - accuracy: 0.6604\n",
            "Epoch 00015: val_loss did not improve from 1.33471\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.3297 - accuracy: 0.6604 - val_loss: 1.3583 - val_accuracy: 0.6667\n",
            "Epoch 16/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.1488 - accuracy: 0.7100\n",
            "Epoch 00016: val_loss improved from 1.33471 to 1.23645, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.1488 - accuracy: 0.7100 - val_loss: 1.2365 - val_accuracy: 0.6984\n",
            "Epoch 17/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.0890 - accuracy: 0.7255\n",
            "Epoch 00017: val_loss did not improve from 1.23645\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.0890 - accuracy: 0.7255 - val_loss: 1.2648 - val_accuracy: 0.6912\n",
            "Epoch 18/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.1668 - accuracy: 0.7061\n",
            "Epoch 00018: val_loss did not improve from 1.23645\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 1.1668 - accuracy: 0.7061 - val_loss: 1.3142 - val_accuracy: 0.6784\n",
            "Epoch 19/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 1.0470 - accuracy: 0.7343\n",
            "Epoch 00019: val_loss improved from 1.23645 to 1.20901, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 1.0470 - accuracy: 0.7343 - val_loss: 1.2090 - val_accuracy: 0.6962\n",
            "Epoch 20/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.9895 - accuracy: 0.7489\n",
            "Epoch 00020: val_loss improved from 1.20901 to 1.16493, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 0.9895 - accuracy: 0.7489 - val_loss: 1.1649 - val_accuracy: 0.7117\n",
            "Epoch 21/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.9581 - accuracy: 0.7608\n",
            "Epoch 00021: val_loss improved from 1.16493 to 1.16108, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 78ms/step - loss: 0.9581 - accuracy: 0.7608 - val_loss: 1.1611 - val_accuracy: 0.7145\n",
            "Epoch 22/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.7695\n",
            "Epoch 00022: val_loss improved from 1.16108 to 1.14751, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 78ms/step - loss: 0.9130 - accuracy: 0.7695 - val_loss: 1.1475 - val_accuracy: 0.7273\n",
            "Epoch 23/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.8789 - accuracy: 0.7773\n",
            "Epoch 00023: val_loss improved from 1.14751 to 1.13447, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 5s 80ms/step - loss: 0.8789 - accuracy: 0.7773 - val_loss: 1.1345 - val_accuracy: 0.7307\n",
            "Epoch 24/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.8639 - accuracy: 0.7811\n",
            "Epoch 00024: val_loss did not improve from 1.13447\n",
            "57/57 [==============================] - 5s 82ms/step - loss: 0.8639 - accuracy: 0.7811 - val_loss: 1.1698 - val_accuracy: 0.7223\n",
            "Epoch 25/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.9666 - accuracy: 0.7570\n",
            "Epoch 00025: val_loss improved from 1.13447 to 1.13244, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 5s 81ms/step - loss: 0.9666 - accuracy: 0.7570 - val_loss: 1.1324 - val_accuracy: 0.7307\n",
            "Epoch 26/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.8323 - accuracy: 0.7887\n",
            "Epoch 00026: val_loss improved from 1.13244 to 1.12312, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 77ms/step - loss: 0.8323 - accuracy: 0.7887 - val_loss: 1.1231 - val_accuracy: 0.7323\n",
            "Epoch 27/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.7989\n",
            "Epoch 00027: val_loss improved from 1.12312 to 1.11125, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 78ms/step - loss: 0.7929 - accuracy: 0.7989 - val_loss: 1.1112 - val_accuracy: 0.7357\n",
            "Epoch 28/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.7647 - accuracy: 0.8039\n",
            "Epoch 00028: val_loss improved from 1.11125 to 1.08721, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 5s 80ms/step - loss: 0.7647 - accuracy: 0.8039 - val_loss: 1.0872 - val_accuracy: 0.7462\n",
            "Epoch 29/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.7240 - accuracy: 0.8148\n",
            "Epoch 00029: val_loss improved from 1.08721 to 1.08553, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 79ms/step - loss: 0.7240 - accuracy: 0.8148 - val_loss: 1.0855 - val_accuracy: 0.7385\n",
            "Epoch 30/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.7186 - accuracy: 0.8127\n",
            "Epoch 00030: val_loss improved from 1.08553 to 1.07554, saving model to best-reuters-lstm.h5\n",
            "57/57 [==============================] - 4s 75ms/step - loss: 0.7186 - accuracy: 0.8127 - val_loss: 1.0755 - val_accuracy: 0.7429\n",
            "Epoch 31/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.6920 - accuracy: 0.8225\n",
            "Epoch 00031: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 77ms/step - loss: 0.6920 - accuracy: 0.8225 - val_loss: 1.0894 - val_accuracy: 0.7490\n",
            "Epoch 32/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.8230\n",
            "Epoch 00032: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 79ms/step - loss: 0.6959 - accuracy: 0.8230 - val_loss: 1.1218 - val_accuracy: 0.7351\n",
            "Epoch 33/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.8305\n",
            "Epoch 00033: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 77ms/step - loss: 0.6670 - accuracy: 0.8305 - val_loss: 1.0957 - val_accuracy: 0.7485\n",
            "Epoch 34/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.8374\n",
            "Epoch 00034: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 5s 80ms/step - loss: 0.6263 - accuracy: 0.8374 - val_loss: 1.1109 - val_accuracy: 0.7307\n",
            "Epoch 35/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.6146 - accuracy: 0.8412\n",
            "Epoch 00035: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 78ms/step - loss: 0.6146 - accuracy: 0.8412 - val_loss: 1.1005 - val_accuracy: 0.7379\n",
            "Epoch 36/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8500\n",
            "Epoch 00036: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 76ms/step - loss: 0.5956 - accuracy: 0.8500 - val_loss: 1.0964 - val_accuracy: 0.7551\n",
            "Epoch 37/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8600\n",
            "Epoch 00037: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 0.5568 - accuracy: 0.8600 - val_loss: 1.1175 - val_accuracy: 0.7529\n",
            "Epoch 38/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.5805 - accuracy: 0.8543\n",
            "Epoch 00038: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 0.5805 - accuracy: 0.8543 - val_loss: 1.0964 - val_accuracy: 0.7457\n",
            "Epoch 39/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.8653\n",
            "Epoch 00039: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 74ms/step - loss: 0.5288 - accuracy: 0.8653 - val_loss: 1.1264 - val_accuracy: 0.7435\n",
            "Epoch 40/50\n",
            "57/57 [==============================] - ETA: 0s - loss: 0.5228 - accuracy: 0.8672\n",
            "Epoch 00040: val_loss did not improve from 1.07554\n",
            "57/57 [==============================] - 4s 73ms/step - loss: 0.5228 - accuracy: 0.8672 - val_loss: 1.1273 - val_accuracy: 0.7474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model(model_path)\n",
        "best_model.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmnf5OAM8bpk",
        "outputId": "6561dbaf-5eb9-4db6-a672-cd48a26f86f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71/71 [==============================] - 2s 20ms/step - loss: 1.1171 - accuracy: 0.7400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1171225309371948, 0.7399821877479553]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nhkfc6hT8_TH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}